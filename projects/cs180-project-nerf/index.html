<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Final Project: Neural Radiance Field (NeRF) | Ryan Liu's Homepage </title> <meta name="author" content="Ryan Liu"> <meta name="description" content="Rendering Novel 3D Views with NeRF"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ryanliu30.github.io/projects/cs180-project-nerf/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Ryan Liu's Homepage </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Final Project: Neural Radiance Field (NeRF)</h1> <p class="post-description">Rendering Novel 3D Views with NeRF</p> </header> <article> <h1 id="introduction">Introduction</h1> <p>In this project, I explored how to use a neural network to render novel views of a 3D object with the framework of neural radiance field (NeRF).</p> <h1 id="part-1-neural-field">Part 1: Neural Field</h1> <p>Before tackling the harder task of 3D rendering, we can first take a detour to look at the 2D version of NeRF. the Neural Field. While NeRF fits a model to represent the function \(F: (x, y, z, \theta, \phi) \to (r, g, b, \sigma)\), the Neural Field fits a model to represent a 2D image as a function \(F: (x, y) \to (r, g, b)\).</p> <h2 id="network-architecture">Network Architecture</h2> <p>To build a Neural Field model, there are two key components: (1) featurization and (2) encoder.</p> <p>Featurization is done by positional encoding, which maps a coordinate by</p> \[\mathrm{PE}_{L}(x) = [x, \sin(2^0\pi x), \dots, \sin(2^{L-1}\pi x), \cos(2^0\pi x), \dots, \cos(2^{L-1}\pi x)] \in \mathbb R^{2L+1}\] <p>where \(x\in[0, 1]\) must be satisfied to ensure the uniquesness of the encoding. To generalize to higher-dimensional coordinantes, we simple concatenate the outputs of individually encoded results, which gives \(\mathbb R^{d(2L+1)}\) encoded vector.</p> <p>The encoder is implemented by an MLP, with six configurable parameters: input dimension \(d_{i}\), hidden dimension \(d_{h}\), output dimension \(d_{o}\), number of layers \(n\), dropout \(p\), and activation \(\sigma(\cdot)\). The architecture is visualized as follows:</p> <div class="row"> <div class="col-sm"> </div> <div class="col-sm"> <div style="margin: auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/mlp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> </div> </div> <p style="text-align:center;">The block diagram of the MLP used in the project.</p> <p>The encoder used for this part of the project has \(L=10\), \(d_{i}=42\), \(d_{h}=256\), \(d_{o}=3\), \(n=3\), \(p=0\), and activation is SiLU \(\sigma(x)=x\cdot\mathrm{sigmoid}(x)\). To make sure the output is in valid RGB range, the output is passed through a sigmoid to rescale it to \([0, 1]\) range.</p> <h2 id="training-procedure">Training Procedure</h2> <p>There are three major components of training the model: data loading, optimization, and evaluation.</p> <p>For data loading, instead of using the torch <code class="language-plaintext highlighter-rouge">Dataset</code> and <code class="language-plaintext highlighter-rouge">DataLoader</code>, I implemented a custom on-device data iterator. The advantage is that torch <code class="language-plaintext highlighter-rouge">Dataloader</code> is a multi-threaded CPU data loader. As our dataset is small (an image!) and batch size is large (~10K), it is more efficient to keep the entire dataset on the cuda device. The images used for training are shown below:</p> <div class="row"> <div class="col-sm"> <p style="text-align:center;">Fox</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/fox.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <p style="text-align:center;">Memorial glade</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/glade.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p style="text-align:center;">The images used to train the Neural Field model.</p> <p>For optimization, I used the Adam optimizer with learning rate of <code class="language-plaintext highlighter-rouge">1e-2</code>, weight decay of <code class="language-plaintext highlighter-rouge">0</code>, and <code class="language-plaintext highlighter-rouge">beta = [0.9, 0.999]</code>. The batch size is set to <code class="language-plaintext highlighter-rouge">10000</code>. A <code class="language-plaintext highlighter-rouge">StepLR</code> scheduler is also used with step size of <code class="language-plaintext highlighter-rouge">10</code> and gamma of <code class="language-plaintext highlighter-rouge">0.9</code>. The model is trained for <code class="language-plaintext highlighter-rouge">100</code> epochs.</p> <p>For evaluation, two metrics are employed: the MSE and PSNR. PSNR is defined as \(\mathrm{PSNR} = - 10\log_{10}(\mathrm{MSE})\). Notice that PSNR must be calculated <strong>after</strong> MSE aggregation per step since Jensen inequality implies PSNR is overestimated if calculated binned. The model checkpoint is saved along the training procedure and inferred to evaluate the model’s performance more intuitively.</p> <h2 id="results-and-ablations">Results and Ablations</h2> <p>An ablation study is carried out to understand how to train a Neural Field better. The two parameters I chose to ablate on is the number of frequencies \(L\) and the weight decay of the Adam optimizer. The rationale behind the choice is that number of frequencies directly controls the resolution of our featurization, and weight decay regularizes the model to prevent overfitting, whereas overfitting is precisely what we want to achieve with the model.</p> <div class="row"> <div class="col-sm"> <p style="text-align:center;">$$L=5$$</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part1-f5.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <p style="text-align:center;">$$L=8$$</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part1-f8.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <p style="text-align:center;">$$L=10$$</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part1-f10.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="row"> <div class="col-sm"> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part1-glade-f5.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part1-glade-f8.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part1-glade-f10.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="row"> <div class="col-sm"> <p style="text-align:center;">Fox</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/frequency_ablations_training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <p style="text-align:center;">Memorial glade</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/glade_frequency_ablations_training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="row"> <div class="col-sm"> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/frequency_ablations_psnr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/glade_frequency_ablations_psnr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p style="text-align:center;">Ablation on frequency</p> <p>We can see that at the beginning, the one with smaller number of frequencies are significantly worse than the ones with more frequencies. Moreover, we observe a larger gap in terms of PSNR for fox than glade. This is presumably due to more high frequency details in the fox image (hair). This verifies our hypothesis that more frequencies gives higher spatial resolution.</p> <div class="row"> <div class="col-sm"> <p style="text-align:center;">weight decay 0</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part1-wd-0.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <p style="text-align:center;">weight decay 1e-5</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part1-wd-1e-5.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <p style="text-align:center;">weight decay 1e-4</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part1-wd-1e-4.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="row"> <div class="col-sm"> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part1-glade-wd-0.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part1-glade-wd-1e-5.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part1-glade-wd-1e-4.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="row"> <div class="col-sm"> <p style="text-align:center;">Fox</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/weight_decay_ablations_training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <p style="text-align:center;">Memorial glade</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/glade_weight_decay_ablations_training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="row"> <div class="col-sm"> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/weight_decay_ablations_psnr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/glade_weight_decay_ablations_psnr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p style="text-align:center;">Ablation on weight decay</p> <p>We can see that weight decay significantly impacted the model’s ability to “overfit” to the image as expected.</p> <h1 id="part-2-neural-radiance-field">Part 2: Neural Radiance Field</h1> <p>Now we can proceed to use Neural Radiance Field (NeRF) to model 3D objects. As described in introduction, NeRF represents the scene with \(F: (x, y, z, \theta, \phi) \to (r, g, b, \sigma)\). However, we do not observe the world as a 3D volume. Instead, we see 2D renders of the scence. Therefore, we need to render the views during traning and inference time, which requires some theory to be worked out.</p> <h2 id="theory">Theory</h2> <p>Our goal is to convert a pixel location \((u, v)\) into world coordinates \((x_w, y_w, z_w)\) along the ray. First, we define the pixel-to-ray transformation:</p> \[\left[ \begin{matrix} su\\ sv\\ s \end{matrix} \right] = \left[ \begin{matrix} f_x &amp; 0 &amp; o_x\\ 0 &amp; f_y &amp; o_y\\ 0 &amp; 0 &amp; 1 \end{matrix} \right]\left[ \begin{matrix} x_c\\ y_c\\ z_c \end{matrix} \right]\] <p>Therefore by inverting the intrinsic matrix \(K=\left[ \begin{matrix} f_x &amp; 0 &amp; o_x\\ 0 &amp; f_y &amp; o_y\\ 0 &amp; 0 &amp; 1 \end{matrix} \right]\), we arrive at the pixel-to-ray transformation:</p> \[\left[ \begin{matrix} x_c\\ y_c\\ z_c \end{matrix} \right] = s^{-1} \left[ \begin{matrix} f_x &amp; 0 &amp; o_x\\ 0 &amp; f_y &amp; o_y\\ 0 &amp; 0 &amp; 1 \end{matrix} \right]^{-1}\left[ \begin{matrix} u\\ v\\ 1 \end{matrix} \right]\] <p>where \(u\) and \(v\) are the center of the pixel, and \(s\) is the parameter that parametrize the ray. We can further transform the coordinantes in camera frame into world frame by using the equation:</p> \[\left[ \begin{matrix} x_c\\ y_c\\ z_c\\ 1 \end{matrix} \right] = \left[ \begin{matrix} \mathbf{R}_{3\times3} &amp; \mathbf t \\ \mathbf{0}_{1\times3} &amp; 1 \end{matrix} \right] \left[ \begin{matrix} x_w\\ y_w\\ z_w\\ 1 \end{matrix} \right]\] <p>Inverting it we get:</p> \[\left[ \begin{matrix} x_w\\ y_w\\ z_w\\ 1 \end{matrix} \right] = \left[ \begin{matrix} \mathbf{R}_{3\times3}^{-1} &amp; -\mathbf{R}_{3\times3}^{-1}\mathbf t \\ \mathbf{0}_{1\times3} &amp; 1 \end{matrix} \right] \left[ \begin{matrix} x_c\\ y_c\\ z_c\\ 1 \end{matrix} \right]\] <p>Notice that with \((x_c, y_c, z_c)=(0, 0, 0)\), we have \((x_w, y_w, z_w)=-\mathbf{R}_{3\times3}^{-1}\mathbf t\). Define the origin of the ray as:</p> \[\mathbf{R}_o=-\mathbf{R}_{3\times3}^{-1}\mathbf t\] <p>Set \(s=1\) and apply the coordinate transformation, we can get a point along the ray direction \(\mathbf{X}_w\). Then we can define the ray direction vector by:</p> \[\mathbf{R}_d = \frac{\mathbf{R}_d - \mathbf{R}_o}{\Vert\mathbf{R}_d - \mathbf{R}_o\Vert_2}\] <p>Using these two vectors, we can efficiently sample points along the ray by \(\mathbf{X}_t = \mathbf{R}_o + t\mathbf{R}_d\)</p> <p>Suppose that we know the radiance field along the ray. How can we render the pixel values correpsonding to the ray? This requires volume rendering, which is defined as follows:</p> \[C(\mathbf{r}) = \int_{t_i}^{t_f} T(t)\sigma(\mathbf{r}(t))c(\mathbf{r}(t), \mathbf{d})\mathrm d t\] <p>where \(T(t):=e^{-\int_{t_i}^t\sigma(\mathbf{r}(t'))\mathrm d t'}\). We can discretize the integral and get an estimator:</p> \[\hat C(\mathbf{r}) = \sum_{i=1}^N T_i(1 - e^{-\sigma_i \delta_t})\mathbf{c}_i\] <p>where \(T_i = e^{-\sum_{j=1}^{i-1}\sigma_i \delta_t}\).</p> <h2 id="network-architecture-1">Network Architecture</h2> <p>Similar to Neural Field, the two main components are still featurization and encoder. As the input is now a five-tuple of \((x, y, z, \theta, \phi)\), we need to rethink the way to featurize it. \((\theta, \phi)\) can be equivalently represented by a three-unit-vector \(\mathbf{R}_d\), which is the ray direction. Therefore, we use two positional encoders to encode them. Notice that \((\theta, \phi)\) does not require a high spatial resolution as it always lies on an unit sphere. Therefore we use \(L=10\) for \((x, y, z)\) and \(L=4\) for ray directions.</p> <p>Another important design choice is to make \(\sigma\) <strong>independent</strong> of ray direction. This essentially requires transparency to be isotropy. This holds true for most materials with very few exceptions, and LEGO is certainly not one of them. The final architecture design is showned below:</p> <div class="row"> <div class="col-sm"> </div> <div class="col-sm"> <div style="margin: auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/nerf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> </div> </div> <p style="text-align:center;">The block diagram of the network used for NeRF</p> <p>The MLP has hidden dimensions of \(256\) and the inputs are concateneted when there are multiple inputs to the MLP. The first two blocks have \(3\) layers, and the output blocks have \(1\) layer (\(L\)-layered MLP has \(L+1\) linear layers).</p> <h2 id="training-procedure-1">Training Procedure</h2> <p>For data loading, we use the theory developed in the theory section and preprocess rays into the vectors to represent them. Given 100 images with \(200\times200\) resolution, there are 4,000,000 rays in total. Though it seems to be a large number, it can fit into the cuda device and for the same reason it will be more efficient to store them on-device. For each batch, we sample points along ray by creating a evenly spaced sequence of \(t\) from \(2.0\) to \(6.0\) with <code class="language-plaintext highlighter-rouge">torch.linspace(2.0, 6.0, 64)</code>. Gaussian noise with \(\sigma=0.01\) is added to the sampled \(t\). The sampled points and the correpsonding pixel values are returned. An example of 100 rays sampled is shown below:</p> <div class="row"> <div class="col-sm"> <div style="margin: auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/render.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p style="text-align:center;">100 random rays and the sampled points.</p> <p>For each pair of ray and pixel values, we evaluate the model on these points and use volume render equations to get the predicted pixel values and train with MSE loss. For the default configuation (from spec), the optimizer is Adam with a learning rate of <code class="language-plaintext highlighter-rouge">5e-4</code> and no weight decay. No learning rate scheduler is used, and the batch size is <code class="language-plaintext highlighter-rouge">10000</code>. The model is trained for <code class="language-plaintext highlighter-rouge">100</code> epochs, which took about 1.5 hours on one RTX 3090. As one of the Bells &amp; Whistles (PSNR &gt; 30), I also experiemented with advance learning rate schedule. This includes starting with a higher learninng rate of <code class="language-plaintext highlighter-rouge">8e-4</code> and using a <code class="language-plaintext highlighter-rouge">CosineAnnelaingLR</code> scheduler with <code class="language-plaintext highlighter-rouge">eta_min</code> of <code class="language-plaintext highlighter-rouge">1e-5</code>. The parameters are not tuned, and the rationale behind the choice is to cover a wider range of learning rate to encourage learning at different scales. This model is labeled “advance” in the results section.</p> <p>To evaluate the model, validation loss and PSNR are reported. I also rendered validation views to provide more intuitive assessment to the model’s performance.</p> <h2 id="results">Results</h2> <p>Firstly, the training and validation curves of NeRF is shown as follows:</p> <div class="row"> <div class="col-sm"> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/training_loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/training_psnr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="row"> <div class="col-sm"> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/validation_loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/validation_psnr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p style="text-align:center;">The training and validation curves of NeRF.</p> <p>We see that both model achieved high PSNR. Moreover, the advance one achived a PSNR higher than 30. To visually assess the model’s performance, we can visualize the validation views throughout the training process.</p> <div class="row"> <div class="col-sm"> </div> <div class="col-sm"> <p style="text-align:center;">Epoch 0: Default</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-default/epoch_0/camera_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <p style="text-align:center;">Epoch 0: Advance</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-advance/epoch_0/camera_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> </div> </div> <div class="row"> <div class="col-sm"> </div> <div class="col-sm"> <p style="text-align:center;">Epoch 1: Default</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-default/epoch_1/camera_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <p style="text-align:center;">Epoch 1: Advance</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-advance/epoch_1/camera_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> </div> </div> <div class="row"> <div class="col-sm"> </div> <div class="col-sm"> <p style="text-align:center;">Epoch 2: Default</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-default/epoch_2/camera_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <p style="text-align:center;">Epoch 2: Advance</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-advance/epoch_2/camera_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> </div> </div> <div class="row"> <div class="col-sm"> </div> <div class="col-sm"> <p style="text-align:center;">Epoch 3: Default</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-default/epoch_3/camera_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <p style="text-align:center;">Epoch 3: Advance</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-advance/epoch_3/camera_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> </div> </div> <div class="row"> <div class="col-sm"> </div> <div class="col-sm"> <p style="text-align:center;">Epoch 4: Default</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-default/epoch_4/camera_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <p style="text-align:center;">Epoch 4: Advance</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-advance/epoch_4/camera_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> </div> </div> <div class="row"> <div class="col-sm"> </div> <div class="col-sm"> <p style="text-align:center;">Epoch 10: Default</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-default/epoch_10/camera_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <p style="text-align:center;">Epoch 10: Advance</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-advance/epoch_10/camera_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> </div> </div> <div class="row"> <div class="col-sm"> </div> <div class="col-sm"> <p style="text-align:center;">Epoch 50: Default</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-default/epoch_50/camera_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <p style="text-align:center;">Epoch 50: Advance</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-advance/epoch_50/camera_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> </div> </div> <div class="row"> <div class="col-sm"> </div> <div class="col-sm"> <p style="text-align:center;">Epoch 99: Default</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-default/epoch_99/camera_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <p style="text-align:center;">Epoch 99: Advance</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-advance/epoch_99/camera_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> </div> </div> <p>The difference between the two are visually marginal.</p> <p>Finally, we can render novel views with the test set:</p> <div class="row"> <div class="col-sm"> <p style="text-align:center;">Default</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-default_epoch_99.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> <p style="text-align:center;">Advance</p> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-advance_epoch_99.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p style="text-align:center;">Rendering results on test set</p> <p>We see both model generates good 3D views.</p> <h2 id="rendering-with-background-colors">Rendering with Background colors</h2> <p>Finally, we can try changing the background colors. Notice that in the volume rendering equation, while a ray with many points of high \(\sigma_i\) would have very small \(T_i\), the probability of the ray terminating, at the end, it is generally not guaranteed that this will always be the case. If we think of volume rendering as an weighted average, then with \(T_N\neq 0\) we are essentially getting a point with weight \(1 - T_N\) with RGB all being zero. This is the black background we are getting. We can render the background with different color by adding \((1-T_N)*\mathbf{c}\) to the volume rendering process. This can be easily implemented by appending <code class="language-plaintext highlighter-rouge">torch.inf</code> to the \(sigma\) array and the desired background color to the \(rgb\) array. An example of rendering with background is shown below:</p> <div class="row"> <div class="col-sm"> </div> <div class="col-sm"> <div style="margin:0 auto;"> <figure> <picture> <img src="/assets/img/180-project-nerf/part2-background_epoch_99.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm"> </div> </div> <p style="text-align:center;">Rendering with another background color</p> <p>We see that there are some black noise in the background. This is due to the fact that if the model predicts black with high sigma it can still accurately describe the training image. This can be resolved by using PNG images with proper alpha channels.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Ryan Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?06cae41083477f121be8cd9797ad8e2f"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"Some cool projects did in class",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"Selected Github repositories",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in '_pages/cv.md'. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:"Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra",description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:"Displaying External Posts on Your al-folio Blog",description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"projects-final-project-neural-radiance-field-nerf",title:"Final Project: Neural Radiance Field (NeRF)",description:"Rendering Novel 3D Views with NeRF",section:"Projects",handler:()=>{window.location.href="/projects/cs180-project-nerf/"}},{id:"projects-project-1-images-of-the-russian-empire",title:"Project 1: Images of the Russian Empire",description:"Colorizing the Prokudin-Gorskii photo collection",section:"Projects",handler:()=>{window.location.href="/projects/cs180-project1/"}},{id:"projects-project-2-fun-with-filters-and-frequencies",title:"Project 2: Fun with Filters and Frequencies!",description:"Image morphing, blending, and more!",section:"Projects",handler:()=>{window.location.href="/projects/cs180-project2/"}},{id:"projects-project-3-face-morphing",title:"Project 3: Face Morphing!",description:"Morphing faces with affine wrapping and cross dissolve",section:"Projects",handler:()=>{window.location.href="/projects/cs180-project3/"}},{id:"projects-project-4-image-warping-and-mosaicing",title:"Project 4: Image Warping and Mosaicing!",description:"Stitching Photo Mosaics",section:"Projects",handler:()=>{window.location.href="/projects/cs180-project4/"}},{id:"projects-project-5-fun-with-diffusion-models",title:"Project 5: Fun With Diffusion Models!",description:"Play around with pretrained diffusion models and train one from scratch!",section:"Projects",handler:()=>{window.location.href="/projects/cs180-project5/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%69%75%72%79%61%6E%33%30@%62%65%72%6B%65%6C%65%79.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=SB_HWdIAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ryanliu30","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/ryan-liu-4785441bb","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>